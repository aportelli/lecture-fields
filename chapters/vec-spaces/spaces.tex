% !TEX root = ../../fields.tex
%% Brian Lecture 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Review of linear vector spaces}
A vector space $V$ is a set of vectors which obey the following rules for addition and
multiplication by scalars
\begin{eqnarray*}
	\v{a} + \v{b}&\in& V \qquad\mbox{ if}\quad \v{a},\v{b} \in V \\
	\alpha \v{a}&\in& V \qquad\mbox{ if}\quad\v{a} \in V \\
	\alpha(\v{a}+\v{b}) &=& \alpha \v{a} + \alpha \v{b}\\
	(\alpha+\beta)\v{a} &=& \alpha \v{a} + \beta \v{a}
\end{eqnarray*}
The space contains a unique zero vector or null vector $\v{0}$, such that
$\v{a}+\v{0}=\v{a}$ for all vectors $\v{a}$.

The set $\mathbb{R}^d$, of all $d$-uples $(x_1,\dots,x_d)$ of real numbers obey these
axioms, and it is the space of $d$-dimensional vectors with real components.
$\mathbb{R}^3$ is the set of three-dimensional vectors common in physics. Other simple
examples are a plane through the origin which forms a two-dimensional space and a line
through the origin which forms a one-dimensional space.

\subsection{Linear independence}
Let $\v{a}$ and $\v{b}$ be two vectors in a plane through the origin, and consider the
equation
\begin{center}
	\bigbox{$\alpha\v{a} + \beta\v{b}=\v{0}$}
\end{center}
If this is satisfied for \emph{non-zero} $\alpha$ and $\beta$ then
$\v{a}$ and $\v{b}$ are said to be \emph{linearly dependent},
\[
	\mbox{\emph{i.e.}} \quad \v{b} = -\frac{\alpha}{\beta}\,\v{a}\;.
\]
Clearly $\v{a}$ and $\v{b}$ are \emph{collinear} (either parallel or antiparallel). If
this equation can be satisfied \emph{only} for $\alpha = \beta = 0$, then $\v{a}$ and
$\v{b}$ are \emph{linearly independent}; they are obviously \emph{not} collinear, and no
$\lambda$ can be found such that $\v{b} = \lambda\v{a}$.

\subsubsection{Notes}
\begin{enumerate}
	\item If $\v{a}$, $\v{b}$ are linearly independent then any vector $\v{r}$ in the plane
	      containing them can be written uniquely as a linear combination
	      \[
		      \v{r}=\alpha \v{a} + \beta \v{b}
	      \]

	\item We say $(\v{a}$, $\v{b})$ \emph{spans} the plane, or that $(\v{a}$, $\v{b})$ forms a
	      \emph{basis} for the plane.

	\item We call $(\alpha,\beta)$ a \emph{representation} of $\v{r}$ in the basis $(\v{a},\v{b})$,
	      and we say that $\alpha$, $\beta$ are the \emph{components} of $\v{r}$ in this basis.
\end{enumerate}
Three vectors are linearly dependent if we
can find non-trivial $\alpha$, $\beta$, $\gamma$ (\ie~not all
zero) such that
\begin{center}
	\bigbox{$\alpha\v{a} + \beta\v{b} +\gamma\v{c} = 0$}
\end{center}
otherwise $\v{a}$, $\v{b}$, $\v{c}$ are linearly independent (no one is a
linear combination of the other two).

\subsubsection{Notes}
\begin{enumerate}
	\item
	      If $\v{a}$, $\v{b}$ and $\v{c}$ are linearly independent they span $\thickR^3$ and
	      form a basis, \emph{i.e.}~for any three-dimensional vector $\vr$ we can find
	      scalars $\alpha$, $\beta$, $\gamma$ such that
	      \[
		      \v{r} = \alpha\v{a} + \beta\v{b} + \gamma\v{c}\;.
	      \]

	\item
	      The \emph{triple} of numbers ($\alpha, \beta, \gamma$) is the
	      \emph{representation} of $\v{r}$ in this basis, and
	      $\alpha$,\ $\beta$,\ $\gamma$ are the \emph{components} of $\v{r}$ in
	      this basis.

	\item
	      The geometrical interpretation of linear dependence in three
	      dimensions is that
	      \begin{center}
		      \bigbox{three linearly dependent vectors $~\Leftrightarrow$ three
			      coplanar vectors}
	      \end{center}

	      To see this, note that if $\alpha\v{a} + \beta\v{b} +\gamma\v{c} = 0$ then
	      \begin{eqnarray*}
		      \mbox{for } \alpha \neq 0:
		      &&
		      \v{a}\cdot (\v{b} \times \v{c}) = 0
		      \qquad\quad\;\; \Rightarrow  \quad
		      \v{a},\, \v{b},\, \v{c} \quad \mbox{are coplanar}\\
		      \mbox{for } \alpha =  0:
		      &&
		      \mbox{$\v{b}$ is collinear with $\v{c}$}
		      \quad \Rightarrow  \quad
		      \v{a},\, \v{b},\, \v{c} \quad \mbox{are coplanar}
	      \end{eqnarray*}

	      The converse (that three co-planar vectors are linearly dependent) follows automatically
	      from the definition of linear dependence. Thus, $\stpr{a}{b}{c} =0$ is a necessary and
	      sufficient condition for three vectors to be linearly dependent in 3 dimensions

	\item Generally, in order to find the components $\alpha$, $\beta$, $\gamma$ one can use
	      \[
		      \alpha \,=\, \vr\cdot\v{a}^*\quad   \beta  = \vr\cdot\v{b}^* \quad \gamma = \vr\cdot\v{c}^*
	      \]
	      where the reciprocal basis vectors $\v{a}^*,\ \v{b}^*,\ \v{c}^*$ are defined by
	      \[
		      \v{a}^* \,=\, \frac{ \v{b} \times \v{c}}{ \v{a}\cdot(\v{b} \times \v{c})}
		      \quad
		      \v{b}^* \,=\, \frac{ \v{c} \times \v{a}}{ \v{b}\cdot(\v{c} \times \v{a})}
		      \quad
		      \v{c}^* \,=\, \frac{ \v{a} \times \v{b}}{ \v{c}\cdot(\v{a} \times \v{b})}
	      \]
	      You should verify this by substitution. Question sheet 2 explores reciprocal vectors
	      further. They are of prime importance in crystallography, for example in describing the
	      positions of sites on a non-cubic lattice.

\end{enumerate}
These ideas can be generalised to vector spaces of arbitrary dimension. For a space of
dimension $d$ one can find at most $d$ linearly independent vectors (this property is in
fact the formal definition of the dimension of a vector space).

\subsection{Standard orthonormal basis: Cartesian basis}
A basis in which the basis vectors are orthogonal is called an \emph{orthogonal} basis,
if additionally all vectors are unit vectors, it is called an \emph{orthonormal} basis.
You have already encountered the idea of \emph{Cartesian coordinates} in which points in
space are labelled by coordinates $(x,y,z)$. We introduce orthonormal basis vectors
denoted by $\ex=(1,0,0)$ $\ey=(0,1,0)$ and $\ez=(0,0,1)$ which point along the $x$, $y$
and $z$-axes, respectively. In the ``$x y z$'' notation the components of a vector
$\v{a}$ are $a_x$, $a_y$, $a_z$, and a vector is written in terms of the basis vectors as
\[
	\v{a}  =  (a_x,a_y,a_z)=\ad{x}\,\ex + \ad{y}\,\ey + \ad{z}\,\ez\;.
\]

\subsection{Introduction to suffix or index notation}
A more systematic labelling of orthonormal basis vectors for $\thickR^3$ is to use
$\eone,\ \etwo$ and $\ethree$. Instead of $\ex$ we write $\eone$, instead of $\ey$ we
write $\etwo$, and instead of $\ez$ we write $\ethree$. Then, from the definition of the
scalar product in Section~(\ref{sec:scalarproduct}), we get
\begin{equation}\label{triad}
	\bigbox{$
			\ds\eone\cdot\eone = \etwo\cdot\etwo = \ethree\cdot\ethree = 1
			\qquad\mbox{and}\qquad
			\eone\cdot\etwo = \etwo\cdot\ethree = \ethree\cdot\eone = 0
		$}
\end{equation}
Similarly the components of any vector $\v{a}$ in 3-d space are denoted by $a_1$, $a_2$
\mbox{and $a_3$}. This scheme is known as the \emph{suffix} or \emph{index} notation. Its
great advantages over `$x y z$' notation are that it clearly generalises easily to any
number of dimensions, and it greatly simplifies manipulations and the verification of
various identities (see later in the course). Thus, any vector $\v{a}$ is written in this
new notation as
\[
	\v{a}
	\,=\,
	a_1 \;\eone + a_2\;\etwo + a_3\; \ethree \\
	\,=\,
	\sum_{i=1}^3\; \ad{i}\;\ei\;.
\]
The last summation will often be abbreviated to $\v{a} =\sum_{i} \ad{i}\;\ei$.
\subsubsection{Notes}
\begin{enumerate}

	\item
	      The three numbers $\ad{i}$, $i=1,\,2,\,3,$ are called the (Cartesian)
	      \emph{components} of $\v{a}$ with respect to the basis set $\{\ei\}$.

	\item
	      We may write $\v{a} = \sum_{i=1}^3\ad{i}\, \ei =
		      \sum_{j=1}^3\ad{j}\,\ej =\sum_{p=1}^3 \ad{p}\,\ed{p} =
		      \sum_{\alpha=1}^3 \ad{\alpha}\,\ed{\alpha}$ where the summed indices
	      $i,\, j, \, p$, $\alpha$ are called `dummy', `repeated' or `summation'
	      indices. We can choose \emph{any} letter or symbol for them.

	\item
	      The components $a_i$ of a vector $\v{a}$ may be obtained using the
	      orthonormality properties of equation~(\ref{triad}):
	      \[
		      \v{a}\cdot\eone
		      =
		      (a_1\,\eone + a_2\,\etwo + a_3\,\ethree)\cdot\eone  =  a_1
	      \]
	      \begin{center}
		      \bigbox{$a_1$ is the projection of $\v{a}$ in the direction of $\eone$.}
	      \end{center}
	      Similarly for the components $a_2$ and $a_3$.  So in general we may
	      write
	      \begin{center}
		      \bigbox{$\v{a}\cdot\ei = \ad{i}\quad\mbox{or sometimes}\quad
				      (\v{a})_i$}
	      \end{center}
	      where in this equation $i$ is a `free' index and may take values
	      $i=1,2,3$.  In this way we are in fact condensing three equations into
	      one.

	\item
	      In terms of these components, the scalar product is
	      \[
		      \v{a}\cdot\v{b}
		      \,=\,
		      (a_1\,\eone + a_2\,\etwo + a_3\,\ethree)
		      \cdot
		      (b_1\,\eone + b_2\,\etwo + b_3\,\ethree)
	      \]
	      Using the orthonormality of the basis vectors (equation~(\ref{triad})), this becomes
	      \begin{center}
		      \bigbox{
			      $\displaystyle\v{a}\cdot\v{b}  =  \sum_{i=1}^3\;a_i \, b_i$
		      }
	      \end{center}
	      In particular the magnitude of a vector is, as expected
	      \[
		      a
		      \,=\,
		      |\v{a}|
		      \,=\,
		      \sqrt{\v{a}\cdot\v{a}}
		      \,=\,
		      \sqrt{a_1^2+a_2^2+a_3^2} \,.
	      \]
\end{enumerate}
\mnote{03L 20/01/09}
