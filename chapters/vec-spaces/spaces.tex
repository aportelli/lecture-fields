% !TEX root = ../../fields.tex
%% Brian Lecture 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Review of linear vector spaces}

Let $V$ denote a linear vector space. Then vectors in $V$ obey the following rules for
addition and multiplication by scalars
\begin{eqnarray*}
	\v{a} + \v{b}&\in& V \qquad\mbox{ if}\quad \v{a},\v{b} \in V \\
	\alpha \v{a}&\in& V \qquad\mbox{ if}\quad\v{a} \in V \\
	\alpha(\v{a}+\v{b}) &=& \alpha \v{a} + \alpha \v{b}\\
	(\alpha+\beta)\v{a} &=& \alpha \v{a} + \beta \v{a}
\end{eqnarray*}
The space contains a zero vector or null vector, $\v{0}$, so that, for
example $\v{a}+(-\v{a})=\v{0}$. We usually omit the underline from the
zero vector.

Of course as we have seen, vectors in $\thickR^3$ (usual 3-dimensional real space) obey
these axioms. Other simple examples are a plane through the origin which forms a
two-dimensional space and a line through the origin which forms a one-dimensional space.

\subsection{Linear independence}

Let $\v{a}$ and $\v{b}$ be two vectors in a plane through the origin, and consider the
equation
\begin{center}
	\bigbox{$\alpha\v{a} + \beta\v{b}$ = 0}
\end{center}
If this is satisfied for \emph{non-zero} $\alpha$ and $\beta$ then
$\v{a}$ and $\v{b}$ are said to be \emph{linearly dependent},
\[
	\mbox{\emph{i.e.}} \quad \v{b} = -\frac{\alpha}{\beta}\,\v{a}\;.
\]
Clearly $\v{a}$ and $\v{b}$ are \emph{collinear} (either parallel or anti-parallel).

If this equation can be satisfied \emph{only} for $\alpha = \beta = 0$, then $\v{a}$ and
$\v{b}$ are \emph{linearly independent}; they are obviously \emph{not} collinear, and no
$\lambda$ can be found such that $\v{b} = \lambda\v{a}$.

\paragraph{Notes}
\begin{enumerate}
	\item If $\v{a},\ \v{b}$ are linearly independent then any vector $\v{r}$ in the plane may be
	      written uniquely as a linear combination
	      \[
		      \v{r}=\alpha \v{a} + \beta \v{b}
	      \]

	\item We say $\v{a},\ \v{b}$ \emph{span} the plane, or $\v{a},\ \v{b}$ form a \emph{basis} for
	      the plane.

	\item We call $(\alpha,\beta)$ a \emph{representation} of $\v{r}$ in the basis formed by
	      $\v{a},\ \v{b}\,$, and we say that $\alpha,\ \beta$ are the \emph{components} of $\v{r}$
	      in this basis.
\end{enumerate}

\textbf{In three dimensions} three vectors are linearly dependent if we
can find non-trivial $\alpha, \beta, \gamma$ (\emph{i.e.}~not all
zero) such that
\begin{center}
	\bigbox{$\alpha\v{a} + \beta\v{b} +\gamma\v{c} = 0$}
\end{center}
otherwise $\v{a},\,\v{b},\,\v{c}$ are linearly independent (no one is a
linear combination of the other two).

	{\bf Notes}
\begin{enumerate}
	\item
	      If $\v{a}$, $\v{b}$ and $\v{c}$ are linearly independent they span
	      $\thickR^3$ and form a basis, \emph{i.e.}~for any vector $\vr$ we can
	      find scalars $\alpha, \beta, \gamma$ such that
	      \[
		      \v{r} = \alpha\v{a} + \beta\v{b} + \gamma\v{c}\;.
	      \]

	\item
	      The \emph{triple} of numbers ($\alpha, \beta, \gamma$) is the
	      \emph{representation} of $\v{r}$ in this basis, and
	      $\alpha$,\ $\beta$,\ $\gamma$ are the \emph{components} of $\v{r}$ in
	      this basis.

	\item
	      The geometrical interpretation of linear dependence in three
	      dimensions is that
	      \begin{center}
		      \bigbox{three linearly dependent vectors $~\Leftrightarrow$~ three
			      coplanar vectors}
	      \end{center}

	      To see this, note that if $\alpha\v{a} + \beta\v{b} +\gamma\v{c} = 0$ then
	      \begin{eqnarray*}
		      \mbox{for } \alpha \neq 0:
		      &&
		      \v{a}\cdot (\v{b} \times \v{c}) = 0
		      \qquad\quad\;\; \Rightarrow  \quad
		      \v{a},\, \v{b},\, \v{c} \quad \mbox{are coplanar}\\
		      \mbox{for } \alpha =  0:
		      &&
		      \mbox{$\v{b}$ is collinear with $\v{c}$}
		      \quad \Rightarrow  \quad
		      \v{a},\, \v{b},\, \v{c} \quad \mbox{are coplanar}
	      \end{eqnarray*}

	      The converse (that three co-planar vectors are linearly dependent) follows automatically
	      from the definition of linear dependence. Thus $\stpr{a}{b}{c} =0$ is a necessary and
	      sufficient condition for 3 vectors to be linearly dependent in 3 dimensions

	\item Generally, in order to find the components $\alpha$, $\beta$, $\gamma$ one can use
	      \[
		      \alpha \,=\, \vr\cdot\v{a}^*\quad   \beta  = \vr\cdot\v{b}^* \quad \gamma = \vr\cdot\v{c}^*
	      \]
	      where the reciprocal basis vectors $\v{a}^*,\ \v{b}^*,\ \v{c}^*$ are defined by
	      \[
		      \v{a}^* \,=\, \frac{ \v{b} \times \v{c}}{ \v{a}\cdot(\v{b} \times \v{c})}
		      \quad
		      \v{b}^* \,=\, \frac{ \v{c} \times \v{a}}{ \v{b}\cdot(\v{c} \times \v{a})}
		      \quad
		      \v{c}^* \,=\, \frac{ \v{a} \times \v{b}}{ \v{c}\cdot(\v{a} \times \v{b})}
	      \]
	      You should verify this by substitution. Question sheet 2 explores reciprocal vectors
	      further. They are of prime importance in crystallography, for example in describing the
	      positions of sites on a non-cubic lattice.

\end{enumerate}

These ideas can be generalised to vector spaces of arbitrary dimension. For a space of
dimension $n$ one can find at most $n$ linearly independent vectors.

%\subsection{Reciprocal basis}

%If $\vL, \vM, \vN$ form a basis for $\thickR^3$ then the components
%of an arbitrary vector 
%\[\vr \ = \ l\vL + m\vM + n\vN\;.\] can be found using
%\[ l = \vr\cdot\vL^*\quad   m = \vr\cdot\vM^* \quad n = \vr\cdot\vN^* \]
%where the reciprocal basis vectors $\vL^*,\ \vM^*,\ \vN^*$ are defined by
%\[
%\vL^*= \frac{ \vM \times \vN}{ \vL\cdot(\vM \times \vN)}
%\quad
%\vM^*= \frac{ \vN \times \vL}{ \vM\cdot(\vN \times \vL)}
%\quad
%\vN^*= \frac{ \vL \times \vM}{ \vN\cdot(\vL \times \vM)}
%\]\\
%You should verify this by substitution. Question 2.5 and 2.6 explore
%reciprocal vectors further. They are of prime importance in
%crystallography, for example  in describing the positions
%of sites on a non-cubic lattice (see Physics 2 Properties of Matter course).

%\subsection{Scalar Product}

%Consider the scalar product of
%$\vr = l\vL + m\vM + n\vN$
%with a different vector
%$\vr' = l'\vL + m'\vM + n'\vN$:
%\begin{eqnarray*}
%\vr\cdot\vr' & = &
%ll'L^2 + mm'M^2 + nn'N^2
%+ (ml' + lm')\vL\cdot\vM\\
%& & + (ln' + nl')\vL\cdot\vN
%+ (mn' + nm')\vM\cdot\vN\,\;.
%\end{eqnarray*}
%For a general basis comprising  \vL, \vM\ and \vN, this is very clumsy.
%It simplifies if the basis vectors are 
%of \emph{unit length} ($L^2 = M^2 = N^2 = 1$) and
%\emph{mutually orthogonal}
%($\vL\cdot\vM = \vL\cdot\vN = \vM\cdot\vN = 0$),
%so that
%\[
%\vr\cdot\vr' \ = \ ll' + mm' + nn'.
%\]
%and the components are easily obtained
%\[
%l= \vr\cdot\vL \quad m= \vr\cdot\vM\quad n= \vr\cdot\vN 
%\]
%This is a very special type of basis that is {\em self-reciprocal} {\it i.e.}
%$\vL^* =\vL$, $\vM^* =\vM$, $\vN^* =\vN$

\subsection{Standard orthonormal basis: Cartesian basis}

A basis in which the basis vectors are \emph{orthogonal} and \emph{normalised} (of unit
length) is called an \textbf{orthonormal} basis.

You have already encountered the idea of \emph{Cartesian coordinates} in which points in
space are labelled by coordinates $(x,y,z)$. We introduce orthonormal basis vectors
denoted by $\ex,\ \ey$\ and $\ez$ which point along the $x$, $y$ and $z$-axes,
respectively.

% It is usually understood that the basis vectors are related by the
% right-hand screw rule, with $\ex \times \ey = \ek$ and so on,
% cyclically.

In the `$x y z$' notation the components of a vector $\v{a}$ are $a_x$, $a_y$, $a_z$, and
a vector is written in terms of the basis vectors as
\[
	\v{a}  \,=\,  \ad{x}\,\ex + \ad{y}\,\ey + \ad{z}\,\ez\,
	\quad\mbox{ (or\ \ }
	\v{a} \,=\, a_x\;\vi + a_y\;\vj + a_z\;\vk\, \mbox{)} \;.
\]
\textbf{NB} We will NOT use the notation `$\vi$, $\vj$, $\vk$' for basis
vectors as this causes confusion in suffix notation.

%Also note that \emph{in this basis} the basis vectors themselves are
%represented by
%\[
%  \vi = \ex = (1, 0, 0) \quad
%  \vj = \ey = (0, 1, 0)  \quad
%  \vk = \ez = (0, 0, 1)
%\]

\subsection{Introduction to suffix or index notation}

A more systematic labelling of orthonormal basis vectors for $\thickR^3$ is to use
$\eone,\ \etwo$ and $\ethree$. Instead of $\ex$ we write $\eone$, instead of $\ey$ we
write $\etwo$, and instead of $\ez$ we write $\ethree$. Then, from the definition of the
scalar product in Section~(\ref{sec:scalarproduct}), we get
\begin{equation}\label{triad}
	\bigbox{$
			\ds\eone\cdot\eone = \etwo\cdot\etwo = \ethree\cdot\ethree = 1
			\qquad\mbox{and}\qquad
			\eone\cdot\etwo = \etwo\cdot\ethree = \ethree\cdot\eone = 0
		$}
\end{equation}

Similarly the components of any vector $\v{a}$ in 3-d space are denoted by $a_1$, $a_2$
\mbox{and $a_3$}.

This scheme is known as the \emph{suffix} or \emph{index} notation. Its great advantages
over `$x y z$' notation are that it clearly generalises easily to any number of
dimensions, and it greatly simplifies manipulations and the verification of various
identities (see later in the course).

\begin{picture}(330,110)(0,-30)
	\put(20,0){\vector(1,0){40}}
	\put(65,-10){\makebox(0,0)[rb]{\mbox{$\vi$}}}
	\put(20,0){\vector(0,1){40}}
	\put(5,40){\makebox(0,0)[lt]{$\vk$}}
	\put(20,0){\vector(3,2){25}}
	\put(38,31){\makebox(0,0)[lt]{$\vj$}}
	\put(20,-30){\makebox(0,0)[lb]{$\vr = x\v{i}+y\v{j} +z\v{k}$}}
	\put(15,50){\makebox(0,0)[lb]{\bf Old notation}}
	\put(80,20){\makebox(0,0)[lb]{\bf or}}
	\put(130,0){\vector(1,0){40}}
	\put(170,-10){\makebox(0,0)[rb]{\mbox{$\ex$}}}
	\put(130,0){\vector(0,1){40}}
	\put(115,40){\makebox(0,0)[lt]{$\ez$}}
	\put(130,0){\vector(3,2){25}}
	\put(148,29){\makebox(0,0)[lt]{$\ey$}}
	\put(130,-30){\makebox(0,0)[lb]{$\vr = x\,\ex+y\,\ey +z\,\ez$}}
	\put(260,0){\vector(1,0){40}}
	\put(310,-10){\makebox(0,0)[rb]{$\eone$}}
	\put(260,0){\vector(0,1){40}}
	\put(245,40){\makebox(0,0)[lt]{$\ethree$}}
	\put(260,0){\vector(3,2){25}}
	\put(278,29){\makebox(0,0)[lt]{$\etwo$}}
	\put(255,50){\makebox(0,0)[lb]{\bf New notation}}
	\put(260,-30){\makebox(0,0)[lb]{$\vr = x_1\,\eone+x_2\,\etwo +x_3\,\ethree$}}
	%%% Roger/Cambridge-ism!                                 \equiv  r_1\,\eone+r_2\,\etwo+r_3\,\ethree$}}
\end{picture}

Thus any vector $\v{a}$ is written in this new notation as
\[
	\v{a}
	\,=\,
	a_1 \;\eone + a_2\;\etwo + a_3\; \ethree \\
	\,=\,
	\sum_{i=1}^3\; \ad{i}\;\ei\;.
\]
\[
	\bigbox{The last summation will often be abbreviated to $\displaystyle
			\v{a} =\sum_{i} \ad{i}\;\ei\ $}
\]

\paragraph{Notes}
\begin{enumerate}

	\item
	      The three numbers $\ad{i}$, $i=1,\,2,\,3,$ are called the (Cartesian)
	      \emph{components} of $\v{a}$ with respect to the basis set $\{\ei\}$.

	\item
	      We may write $\ds \v{a} = \sum_{i=1}^3\ad{i}\, \ei =
		      \sum_{j=1}^3\ad{j}\,\ej =\sum_{p=1}^3 \ad{p}\,\ed{p} =
		      \sum_{\alpha=1}^3 \ad{\alpha}\,\ed{\alpha}$ where the summed indices
	      $i,\, j, \, p$, $\alpha$ are called `dummy', `repeated' or `summation'
	      indices. We can choose \emph{any} letter or symbol for them.

	\item
	      The components $a_i$ of a vector $\v{a}$ may be obtained using the
	      orthonormality properties of equation~(\ref{triad}):
	      \[
		      \v{a}\cdot\eone
		      =
		      (a_1\,\eone + a_2\,\etwo + a_3\,\ethree)\cdot\eone  =  a_1
	      \]
	      \begin{center}
		      \bigbox{$a_1$ is the projection of $\v{a}$ in the direction of $\eone$.}
	      \end{center}
	      Similarly for the components $a_2$ and $a_3$.  So in general we may
	      write
	      \begin{center}
		      \bigbox{$\v{a}\cdot\ei = \ad{i}\quad\mbox{or sometimes}\quad
				      (\v{a})_i$}
	      \end{center}
	      where in this equation $i$ is a `free' index and may take values
	      $i=1,2,3$.  In this way we are in fact condensing three equations into
	      one.

	\item
	      In terms of these components, the scalar product is
	      \[
		      \v{a}\cdot\v{b}
		      \,=\,
		      (a_1\,\eone + a_2\,\etwo + a_3\,\ethree)
		      \cdot
		      (b_1\,\eone + b_2\,\etwo + b_3\,\ethree)
	      \]
	      Using the orthonormality of the basis vectors (equation~(\ref{triad})), this becomes
	      \begin{center}
		      \bigbox{
			      $\displaystyle\v{a}\cdot\v{b}  =  \sum_{i=1}^3\;a_i \, b_i$
		      }
	      \end{center}
	      In particular the magnitude of a vector is now
	      \[
		      a
		      \,=\,
		      |\v{a}|
		      \,=\,
		      \sqrt{\v{a}\cdot\v{a}}
		      \,=\,
		      \sqrt{a_1^2+a_2^2+a_3^2} \,.
	      \]

	      \mnote{03L 15/01/08}

	      % Zap/postpone the next section? No, because Roger refers to the
	      % elements of the transofrmation matrix as direction cosines.

	      % \item
	      % From Notes 3 and 4 above we can define {\bf direction cosines} $l_1$,
	      % $l_2$, $l_3$ of the vector $\v{a}$ as the cosines of the angles
	      % between the vector and the basis axes, namely
	      % \[
	      %    l_i \equiv \cos\theta_i
	      %    \,=\,
	      %    {\v{a}\cdot\v{e_i} \over a}
	      %    \,=\,
	      %    {a_i \over a}\,, \qquad i = 1, 2, 3 \,.  \nonumber
	      % \]
	      % It follows that
	      % \[
	      %   \sum_{i=1}^3 l_i^2 \equiv l_1^2 + l_2^2 + l_3^2
	      %   \,=\,
	      %   1 \,.
	      % \]
	      % If $\v{a}$ has direction cosines $l_i\,$, $\v{b}$ has direction
	      % cosines $m_i\,$, and $\theta$ is the angle between $\v{a}$ and
	      % $\v{b}$, then
	      % \[
	      %   \v{a}\cdot\v{b}
	      %   \,=\,
	      %   \sum_{i=1}^{3} a_i b_i 
	      %   \,=\,
	      %   ab \sum_{i=1}^{3} l_i m_i
	      %   \,=\,
	      %   ab\cos\theta \,,
	      % \]
	      % or
	      % \[
	      %   \cos\theta
	      %   \,=\,
	      %   \sum_{i=1}^{3} l_i m_i \,.
	      % \]

\end{enumerate}
\mnote{03L 20/01/09}
